{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Social Data Science — Group Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: On average, do posts under \"AskMen\" (M), \"AskWomen\" (W), \"TooAfraidtoask\" (A) have different number of comments?\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "|Hypothesis|Elaboration|\n",
    "|:-----|:-----|\n",
    "|Null Hypothesis, $H_0$|$\\mu_M = \\mu_W = \\mu_A$|\n",
    "|Alternative Hypothesis, $H_1$|The mean are non-equal|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Import pre-requisite libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from models.reddit_scraper import RedditScraper\n",
    "from config.settings import USER_AGENT\n",
    "from utils.analysis import *\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "from utils.network_builder import *\n",
    "import time\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scrape and load data from Reddit\n",
    "\n",
    "1. Specify subreddits we are interested in \n",
    "\n",
    "2. For each subreddit, perform the function `scrapesubreddit_convertoDF`\n",
    "\n",
    "    a. Load 30 posts (`numberofposts = 30`)\n",
    "\n",
    "    b. Create a dataframe containing all the posts (`posts_df`)\n",
    "\n",
    "    c. Create a dataframe containing all the comments for all posts under the subreddit (`comments_df`)\n",
    "\n",
    "3. **Output is 6 dataframes**\n",
    "\n",
    "|Label|What it is|\n",
    "|:-----|:-----|\n",
    "|`AskMen_posts_df`|Dataframe containing all the **posts** under subreddit **_AskMen_**|\n",
    "|`AskMen_comments_df`|Dataframe containing all the **comments** under subreddit **_AskMen_**|\n",
    "|`AskWomen_posts_df`|Dataframe containing all the **posts** under subreddit **_AskWomen_**|\n",
    "|`AskWomen_comments_df`|Dataframe containing all the **comments** under subreddit **_AskWomen_**|\n",
    "|`TooAfraidtoask_posts_df`|Dataframe containing all the **posts** under subreddit **_TooAfraidtoask_**|\n",
    "|`TooAfraidtoask_comments_df`|Dataframe containing all the **comments** under subreddit **_TooAfraidtoask_**|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the scraper\n",
    "scraper = RedditScraper(USER_AGENT)\n",
    "\n",
    "# 2. Define function to \n",
    "    # (1) load posts from the subreddit and convert into a dataframe, \n",
    "    # (2) load comments for each post and convert into a dataframe\n",
    "\n",
    "def scrapesubreddit_converttoDF(scraper, subreddit_name, numberofposts):\n",
    "    posts = scraper.get_subreddit_posts(subreddit_name, limit=numberofposts, cache=False, sort=\"top\")\n",
    "    posts_df = pd.DataFrame(posts)\n",
    "\n",
    "        # This is the dataframe for ALL posts under the subreddit\n",
    "\n",
    "    comment_list = []\n",
    "    for post in posts:\n",
    "        comments = scraper.get_post_comments(post['id'])\n",
    "        comment_list.append(pd.DataFrame(comments))\n",
    "        time.sleep(2)\n",
    "\n",
    "    comments_df = pd.concat(comment_list)\n",
    "\n",
    "    return posts_df, comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise number of posts to download\n",
    "\n",
    "numberofposts = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching posts from https://api.reddit.com/r/AskMen/top\n",
      "Response keys: dict_keys(['kind', 'data'])\n",
      "Fetching comments from https://api.reddit.com/comments/1gl5i9p\n",
      "Fetching comments from https://api.reddit.com/comments/1gl8zus\n",
      "Fetching comments from https://api.reddit.com/comments/1gl1xbq\n",
      "Fetching comments from https://api.reddit.com/comments/1glbxkd\n",
      "Fetching comments from https://api.reddit.com/comments/1gl34cb\n",
      "Fetching comments from https://api.reddit.com/comments/1gl5idm\n",
      "Fetching comments from https://api.reddit.com/comments/1gl6qfk\n",
      "Fetching comments from https://api.reddit.com/comments/1gl8uvd\n",
      "Fetching comments from https://api.reddit.com/comments/1gkyor4\n",
      "Fetching comments from https://api.reddit.com/comments/1gli80m\n",
      "Fetching comments from https://api.reddit.com/comments/1glfa1n\n",
      "Fetching comments from https://api.reddit.com/comments/1gl9zut\n",
      "Fetching comments from https://api.reddit.com/comments/1gkz57x\n",
      "Fetching comments from https://api.reddit.com/comments/1gkvznr\n",
      "Fetching comments from https://api.reddit.com/comments/1gl92zb\n",
      "Fetching comments from https://api.reddit.com/comments/1gl4zju\n",
      "Fetching comments from https://api.reddit.com/comments/1gl195g\n",
      "Fetching comments from https://api.reddit.com/comments/1gllxd8\n",
      "Fetching comments from https://api.reddit.com/comments/1glk3l5\n",
      "Fetching comments from https://api.reddit.com/comments/1gl64yc\n",
      "Fetching comments from https://api.reddit.com/comments/1glec8l\n",
      "Fetching comments from https://api.reddit.com/comments/1gl2qp6\n",
      "Fetching comments from https://api.reddit.com/comments/1glh1d2\n",
      "Fetching comments from https://api.reddit.com/comments/1gley1j\n",
      "Fetching comments from https://api.reddit.com/comments/1gldewa\n",
      "Fetching comments from https://api.reddit.com/comments/1gl7n3v\n",
      "Fetching comments from https://api.reddit.com/comments/1gl2qf4\n",
      "Fetching comments from https://api.reddit.com/comments/1gllohz\n",
      "Fetching comments from https://api.reddit.com/comments/1glcurx\n",
      "Fetching comments from https://api.reddit.com/comments/1gll5r0\n"
     ]
    }
   ],
   "source": [
    "# 3a. For subreddit AskMen\n",
    "\n",
    "subreddit_name_1 = \"AskMen\"\n",
    "numberofposts_1 = numberofposts\n",
    "\n",
    "AskMen_posts_df, AskMen_comments_df = scrapesubreddit_converttoDF(scraper, subreddit_name_1, numberofposts_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching posts from https://api.reddit.com/r/AskWomen/top\n",
      "Response keys: dict_keys(['kind', 'data'])\n",
      "Fetching comments from https://api.reddit.com/comments/1gl302u\n",
      "Fetching comments from https://api.reddit.com/comments/1gkyblo\n",
      "Fetching comments from https://api.reddit.com/comments/1glbp1v\n",
      "Fetching comments from https://api.reddit.com/comments/1gl9vci\n",
      "Fetching comments from https://api.reddit.com/comments/1glk82k\n",
      "Fetching comments from https://api.reddit.com/comments/1glbqmm\n",
      "Fetching comments from https://api.reddit.com/comments/1glbnmr\n",
      "Fetching comments from https://api.reddit.com/comments/1gl7r9v\n",
      "Fetching comments from https://api.reddit.com/comments/1glivrt\n",
      "Fetching comments from https://api.reddit.com/comments/1gll8it\n",
      "Fetching comments from https://api.reddit.com/comments/1glfvuz\n",
      "Fetching comments from https://api.reddit.com/comments/1gkx4hy\n"
     ]
    }
   ],
   "source": [
    "# 3b. For subreddit AskWomen\n",
    "\n",
    "subreddit_name_2 = \"AskWomen\"\n",
    "numberofposts_2 = numberofposts\n",
    "\n",
    "AskWomen_posts_df, AskWomen_comments_df = scrapesubreddit_converttoDF(scraper, subreddit_name_2, numberofposts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching posts from https://api.reddit.com/r/TooAfraidtoask/top\n",
      "Response keys: dict_keys(['kind', 'data'])\n",
      "Fetching comments from https://api.reddit.com/comments/1glgh0a\n",
      "Fetching comments from https://api.reddit.com/comments/1gl24fz\n",
      "Fetching comments from https://api.reddit.com/comments/1glgeku\n",
      "Fetching comments from https://api.reddit.com/comments/1gl6vot\n",
      "Fetching comments from https://api.reddit.com/comments/1glimg1\n",
      "Fetching comments from https://api.reddit.com/comments/1gkxpo8\n",
      "Fetching comments from https://api.reddit.com/comments/1gl8vmh\n",
      "Fetching comments from https://api.reddit.com/comments/1gl2hga\n",
      "Fetching comments from https://api.reddit.com/comments/1gle8pm\n",
      "Fetching comments from https://api.reddit.com/comments/1gl6dmx\n",
      "Fetching comments from https://api.reddit.com/comments/1gleh3n\n",
      "Fetching comments from https://api.reddit.com/comments/1gl2ycq\n",
      "Fetching comments from https://api.reddit.com/comments/1gl5ag5\n",
      "Fetching comments from https://api.reddit.com/comments/1glheoi\n",
      "Fetching comments from https://api.reddit.com/comments/1gl5ee7\n",
      "Fetching comments from https://api.reddit.com/comments/1glawll\n",
      "Fetching comments from https://api.reddit.com/comments/1gla235\n",
      "Fetching comments from https://api.reddit.com/comments/1glfd42\n",
      "Fetching comments from https://api.reddit.com/comments/1gliig1\n",
      "Fetching comments from https://api.reddit.com/comments/1gldv2m\n",
      "Fetching comments from https://api.reddit.com/comments/1gllx90\n",
      "Fetching comments from https://api.reddit.com/comments/1gkz4x0\n",
      "Fetching comments from https://api.reddit.com/comments/1glc170\n",
      "Fetching comments from https://api.reddit.com/comments/1gl3zbi\n",
      "Fetching comments from https://api.reddit.com/comments/1gll6df\n",
      "Fetching comments from https://api.reddit.com/comments/1glo9si\n",
      "Fetching comments from https://api.reddit.com/comments/1glgh9a\n",
      "Fetching comments from https://api.reddit.com/comments/1glk147\n",
      "Fetching comments from https://api.reddit.com/comments/1gl4xax\n",
      "Fetching comments from https://api.reddit.com/comments/1glj7tt\n"
     ]
    }
   ],
   "source": [
    "# 3c. For subreddit TooAfraidtoask\n",
    "\n",
    "subreddit_name_3 = \"TooAfraidtoask\"\n",
    "numberofposts_3 = numberofposts\n",
    "\n",
    "TooAfraidtoask_posts_df, TooAfraidtoask_comments_df = scrapesubreddit_converttoDF(scraper, subreddit_name_3, numberofposts_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Find the average depth of ALL comment trees (ie ALL posts) under each subreddit\n",
    "\n",
    "ie Find the average length of all chains from root nodes to leaf nodes across all posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define function to obtain average depth of each comment tree (ie each post)\n",
    "\n",
    "def calculate_average_chain_length(G):\n",
    "    \"\"\"\n",
    "    Calculate the average length of chains of sequential nodes in the directed graph G.\n",
    "    \"\"\"\n",
    "    chain_lengths = []\n",
    "    \n",
    "    # Find all root nodes (nodes with no incoming edges)\n",
    "    for node in G.nodes:\n",
    "        if G.in_degree(node) == 0:\n",
    "            # Calculate the length of each chain starting from the root node\n",
    "            for path in nx.single_source_dijkstra_path_length(G, node):\n",
    "                chain_lengths.append(len(path))\n",
    "    \n",
    "    # Calculate the average chain length\n",
    "    average_length = sum(chain_lengths) / len(chain_lengths) if chain_lengths else 0\n",
    "    return average_length\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define function to obtain average depth of all comment trees (ie all posts) under each subreddit\n",
    "\n",
    "\n",
    "def average_depth_per_subreddit(posts_df, comments_df):\n",
    "\n",
    "    \"\"\"\n",
    "    2a. Iterate the function across ALL posts in the posts_df for each subreddit\n",
    "    \"\"\"\n",
    "    \n",
    "    all_posts_chain_depth = []\n",
    "\n",
    "    for i in posts_df['id']:\n",
    "\n",
    "        post_i_comments = comments_df[comments_df['post_id'] == i]\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        2b. For each post, create a directed network of comments with optional root nodes.\n",
    "        \"\"\"\n",
    "\n",
    "        comment_tree = create_comment_tree(post_i_comments, include_root=True)\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        2c. For each post, calculate the average length of chains of sequential nodes in each comment tree (directed graph G)\n",
    "        \"\"\"\n",
    "\n",
    "        average_depth = calculate_average_chain_length(comment_tree)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        2d. For each post, append the average depth to `all_posts_chain_lengths`\n",
    "        \"\"\"\n",
    "\n",
    "        all_posts_chain_depth.append(average_depth)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    2d. Across all posts (comment trees), calculate the average depth (average length of chains of sequential nodes)\n",
    "    \"\"\"\n",
    "\n",
    "    subreddit_average_depth = sum(all_posts_chain_depth) / len(all_posts_chain_depth) if all_posts_chain_depth else 0\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    2e. Convert the list of average depths across all posts in the subreddit into a DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    subreddit_all_depth_df = pd.DataFrame(all_posts_chain_depth, columns = [\"Depth\"])\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    2f. Return the average depth across all posts of each subreddit, and the dataframe containing all the average depths of each post within the subreddit\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return subreddit_average_depth, subreddit_all_depth_df\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average depth across all posts in subreddit AskMen is 6.98788\n"
     ]
    }
   ],
   "source": [
    "# 3a. Average depth for AskMen\n",
    "\n",
    "AskMen_average_depth, AskMen_all_depth_df = average_depth_per_subreddit(AskMen_posts_df, AskMen_comments_df)\n",
    "\n",
    "print(f\"Average depth across all posts in subreddit AskMen is {AskMen_average_depth:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average depth across all posts in subreddit AskWomen is 5.83333\n"
     ]
    }
   ],
   "source": [
    "# 3b. Average depth for AskWomen\n",
    "\n",
    "AskWomen_average_depth, AskWomen_all_depth_df = average_depth_per_subreddit(AskWomen_posts_df, AskWomen_comments_df)\n",
    "\n",
    "print(f\"Average depth across all posts in subreddit AskWomen is {AskWomen_average_depth:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average depth across all posts in subreddit TooAfraidtoask is 6.99752\n"
     ]
    }
   ],
   "source": [
    "# 3c. Average depth for TooAfraidtoask\n",
    "\n",
    "TooAfraidtoask_average_depth, TooAfraidtoask_all_depth_df = average_depth_per_subreddit(TooAfraidtoask_posts_df, TooAfraidtoask_comments_df)\n",
    "\n",
    "print(f\"Average depth across all posts in subreddit TooAfraidtoask is {TooAfraidtoask_average_depth:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Conduct a one-way ANOVA to test equality in distributions of average depth across posts in each subreddit\n",
    "\n",
    "Null Hypothesis is that all the distributions have the same distribution of average depth across posts in each subreddit\n",
    "\n",
    "Alternative Hypothesis is that the distributions are not identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-way ANOVA Results:\n",
      "F-statistic: 5.673\n",
      "p-value: 0.005\n",
      "==================================================\n",
      "Since p-value = 0.005 < 0.05,\n",
      "          at 5% significance level we reject the null hypothesis\n",
      "          and conclude that there is a statistically significant difference across the distributions\n"
     ]
    }
   ],
   "source": [
    "# 1. Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(AskMen_all_depth_df['Depth'], AskWomen_all_depth_df['Depth'], TooAfraidtoask_all_depth_df['Depth'])\n",
    "\n",
    "significance_level = 0.05\n",
    "\n",
    "# 2a. Print the results\n",
    "print(\"One-way ANOVA Results:\")\n",
    "print(f\"F-statistic: {f_statistic:.3f}\")\n",
    "print(f\"p-value: {p_value:.3f}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 2b. Conditional Conclusion\n",
    "if p_value > significance_level:\n",
    "    print(f'''Since p-value = {p_value:.3f} > 0.05,\n",
    "          at 5% significance level we do not reject the null hypothesis\n",
    "          and conclude that there is no statistically significant difference across the distributions''')\n",
    "    \n",
    "else: \n",
    "    print(f'''Since p-value = {p_value:.3f} < 0.05,\n",
    "          at 5% significance level we reject the null hypothesis\n",
    "          and conclude that there is a statistically significant difference across the distributions''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tukey's HSD Test Results:\n",
      "     Multiple Comparison of Means - Tukey HSD, FWER=0.05     \n",
      "=============================================================\n",
      " group1      group2     meandiff p-adj   lower  upper  reject\n",
      "-------------------------------------------------------------\n",
      "  AskMen       AskWomen  -1.1545 0.0077 -2.0451 -0.264   True\n",
      "  AskMen TooAfraidtoask   0.0096 0.9994 -0.6635 0.6828  False\n",
      "AskWomen TooAfraidtoask   1.1642 0.0071  0.2737 2.0547   True\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Add a column to each dataframe specifying the subreddit\n",
    "\n",
    "AskMen_all_depth_df[\"subreddit\"] = subreddit_name_1\n",
    "\n",
    "AskWomen_all_depth_df[\"subreddit\"] = subreddit_name_2\n",
    "\n",
    "TooAfraidtoask_all_depth_df[\"subreddit\"] = subreddit_name_3\n",
    "\n",
    "\n",
    "# 2. Concatenate all the dataframes together\n",
    "\n",
    "combined_average_depth_df = pd.concat([AskMen_all_depth_df, AskWomen_all_depth_df, TooAfraidtoask_all_depth_df], ignore_index = True)\n",
    "\n",
    "\n",
    "# 3. Conduct Tukey's HSD\n",
    "\n",
    "tukey_results = pairwise_tukeyhsd(combined_average_depth_df['Depth'], combined_average_depth_df['subreddit'])\n",
    "\n",
    "\n",
    "# 4. Print results of Tukey's HSD\n",
    "\n",
    "print(\"\\nTukey's HSD Test Results:\")\n",
    "print(tukey_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since p-value for AskMen-AskWomen and AskWomen-TooAfraidtoask is < 0.05, hence at 5% significance level, we reject the null hypothesis for these 2 cases\n",
    "\n",
    "Since p-value for AskMen-TooAfraidtoask is > 0.05, hence at 5% significance level, we do not reject the null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
